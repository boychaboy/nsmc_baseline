{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scjSguxCes6t"
   },
   "outputs": [],
   "source": [
    "# !pip install mxnet-cu101\n",
    "# !pip install gluonnlp pandas tqdm\n",
    "# !pip install sentencepiece==0.1.85\n",
    "# !pip install transformers==2.1.1\n",
    "# !pip install torch==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scjSguxCes6t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu101\n",
      "  Downloading mxnet_cu101-1.6.0-py2.py3-none-manylinux1_x86_64.whl (710.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 710.5 MB 8.5 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from mxnet-cu101) (1.18.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from mxnet-cu101) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.25.9)\n",
      "Installing collected packages: graphviz, mxnet-cu101\n",
      "Successfully installed graphviz-0.8.4 mxnet-cu101-1.6.0\n",
      "Collecting gluonnlp\n",
      "  Using cached gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "Requirement already satisfied: pandas in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: tqdm in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (4.46.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from gluonnlp) (1.18.5)\n",
      "Collecting cython\n",
      "  Using cached Cython-0.29.20-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: packaging in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from gluonnlp) (20.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from packaging->gluonnlp) (1.15.0)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp38-cp38-linux_x86_64.whl size=501287 sha256=671568010f49623a116f770d45c681015de6a0a5b65e44560a8a8fa26e2a4827\n",
      "  Stored in directory: /home/boychaboy/.cache/pip/wheels/92/fb/71/9b3afdece6b125729ae59b1524d9adcf932d7380e688b449b7\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: cython, gluonnlp\n",
      "Successfully installed cython-0.29.20 gluonnlp-0.9.1\n",
      "Requirement already satisfied: sentencepiece in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (0.1.91)\n",
      "Requirement already satisfied: transformers in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: sacremoses in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: numpy in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (4.46.1)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: filelock in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: click in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: torch in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (1.5.1)\n",
      "Requirement already satisfied: future in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/boychaboy/anaconda3/envs/sa_base/lib/python3.8/site-packages (from torch) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu101\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmrqktyges66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-m31hcvl3\n",
      "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-m31hcvl3\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.1.1-py3-none-any.whl size=12871 sha256=ad21d15c22a91bb880534a1fd4d7afcbf811c112ffebf8d98708110086662e21\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hkd99g66/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_I97zu8es7D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6XUOhjLes7K"
   },
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jZ_p-dKes7P"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# from transformers.optimization import WarmupLinearSchedule\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__D1OErmes7T"
   },
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65J3k4RJes7W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWcVr4WJes7Z"
   },
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "# !wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMsELDsBes7c"
   },
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"nsmc/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"nsmc/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['신카이 마코토의 작화와,미유와 하나카나가 연기를 잘해줘서 더대박이였다.', '1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7X5jiPqes7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4wPpmtces7i"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHLEyqbEes7l"
   },
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGOl8-xZes7n"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFzUfSMces7q"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoCzWBOdes7u"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOIfP1NHes7x"
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tx6gTJZ3es70"
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1aXAtIhes72"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bd5uS_Bes75"
   },
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQv6KAp-es77"
   },
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nichyQIYes79"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YIMGRbfes7_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-e6a38b13095b>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a924ab0896e464698bc330c603888cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7093364000320435 train acc 0.578125\n",
      "epoch 1 batch id 201 loss 0.5101841688156128 train acc 0.5788246268656716\n",
      "epoch 1 batch id 401 loss 0.488308846950531 train acc 0.6778366583541147\n",
      "epoch 1 batch id 601 loss 0.4188087284564972 train acc 0.7266014975041597\n",
      "epoch 1 batch id 801 loss 0.43801769614219666 train acc 0.7558325530586767\n",
      "epoch 1 batch id 1001 loss 0.292766273021698 train acc 0.7750062437562437\n",
      "epoch 1 batch id 1201 loss 0.36145180463790894 train acc 0.7884315154038302\n",
      "epoch 1 batch id 1401 loss 0.3802943229675293 train acc 0.7975999286224126\n",
      "epoch 1 batch id 1601 loss 0.351998895406723 train acc 0.8059123204247346\n",
      "epoch 1 batch id 1801 loss 0.23608028888702393 train acc 0.8128730566352027\n",
      "epoch 1 batch id 2001 loss 0.32831335067749023 train acc 0.8189186656671664\n",
      "epoch 1 batch id 2201 loss 0.27901989221572876 train acc 0.824156633348478\n",
      "\n",
      "epoch 1 train acc 0.8276806029579067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-e6a38b13095b>:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121d3d133aa045f3855b4baeba668028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=782.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 test acc 0.8811141304347826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996c9b2af5d44de3b12563c47ad1155e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.4304448664188385 train acc 0.8125\n",
      "epoch 2 batch id 201 loss 0.19990697503089905 train acc 0.8800528606965174\n",
      "epoch 2 batch id 401 loss 0.263326495885849 train acc 0.8834164588528678\n",
      "epoch 2 batch id 601 loss 0.38143739104270935 train acc 0.88573731281198\n",
      "epoch 2 batch id 801 loss 0.3397142291069031 train acc 0.8880305867665418\n",
      "epoch 2 batch id 1001 loss 0.23441912233829498 train acc 0.8909059690309691\n",
      "epoch 2 batch id 1201 loss 0.19954019784927368 train acc 0.8938514779350542\n",
      "epoch 2 batch id 1401 loss 0.23695439100265503 train acc 0.8968705389007852\n",
      "epoch 2 batch id 1601 loss 0.35101765394210815 train acc 0.8988230012492192\n",
      "epoch 2 batch id 1801 loss 0.18626707792282104 train acc 0.9007756107717935\n",
      "epoch 2 batch id 2001 loss 0.21856847405433655 train acc 0.9027127061469266\n",
      "epoch 2 batch id 2201 loss 0.25592467188835144 train acc 0.9041273852794185\n",
      "\n",
      "epoch 2 train acc 0.9053856477531286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a61f00059b24607b5980231cf1813ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=782.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 test acc 0.8908647698209718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb664e91d1f4eeea9ecf3aa34a3af78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.4523036777973175 train acc 0.84375\n",
      "epoch 3 batch id 201 loss 0.08855131268501282 train acc 0.9237406716417911\n",
      "epoch 3 batch id 401 loss 0.12874463200569153 train acc 0.9245635910224439\n",
      "epoch 3 batch id 601 loss 0.3028908669948578 train acc 0.9268146838602329\n",
      "epoch 3 batch id 801 loss 0.23751571774482727 train acc 0.9287609238451935\n",
      "epoch 3 batch id 1001 loss 0.2585325241088867 train acc 0.9312718531468531\n",
      "epoch 3 batch id 1201 loss 0.119809091091156 train acc 0.934234492089925\n",
      "epoch 3 batch id 1401 loss 0.12343645095825195 train acc 0.9359163097787295\n",
      "epoch 3 batch id 1601 loss 0.15655484795570374 train acc 0.937216973766396\n",
      "epoch 3 batch id 1801 loss 0.13214194774627686 train acc 0.9385150610771793\n",
      "epoch 3 batch id 2001 loss 0.13946442306041718 train acc 0.9400299850074962\n",
      "epoch 3 batch id 2201 loss 0.17118676006793976 train acc 0.9407016696955929\n",
      "\n",
      "epoch 3 train acc 0.94140625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6921a617ba08448dbcf8f4bce2ffbd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=782.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 test acc 0.895400415601023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0d015aea3c4bee936d3e8676f030dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.41443440318107605 train acc 0.859375\n",
      "epoch 4 batch id 201 loss 0.06927099078893661 train acc 0.9561567164179104\n",
      "epoch 4 batch id 401 loss 0.06823982298374176 train acc 0.9555798004987531\n",
      "epoch 4 batch id 601 loss 0.23131349682807922 train acc 0.9561408069883528\n",
      "epoch 4 batch id 801 loss 0.13685646653175354 train acc 0.9575335518102372\n",
      "epoch 4 batch id 1001 loss 0.06695158779621124 train acc 0.9587912087912088\n",
      "epoch 4 batch id 1201 loss 0.05230768769979477 train acc 0.9597991257285595\n",
      "epoch 4 batch id 1401 loss 0.08275384455919266 train acc 0.9608650071377588\n",
      "epoch 4 batch id 1601 loss 0.10209817439317703 train acc 0.9616255465334166\n",
      "epoch 4 batch id 1801 loss 0.08631472289562225 train acc 0.9623993614658523\n",
      "epoch 4 batch id 2001 loss 0.08201278001070023 train acc 0.9632058970514743\n",
      "epoch 4 batch id 2201 loss 0.14189597964286804 train acc 0.9636457860063607\n",
      "\n",
      "epoch 4 train acc 0.96417715443686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecd91107802439ca3c8abe804f9e717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=782.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 test acc 0.8964194373401535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e821ca428340d6bfffa52ca0d049e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2344.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.2862774431705475 train acc 0.921875\n",
      "epoch 5 batch id 201 loss 0.02337711676955223 train acc 0.9735696517412935\n",
      "epoch 5 batch id 401 loss 0.04109572619199753 train acc 0.973698566084788\n",
      "epoch 5 batch id 601 loss 0.27604761719703674 train acc 0.9742096505823628\n",
      "epoch 5 batch id 801 loss 0.0688333585858345 train acc 0.9746215667915106\n",
      "epoch 5 batch id 1001 loss 0.009490116499364376 train acc 0.9753996003996004\n",
      "epoch 5 batch id 1201 loss 0.010796567425131798 train acc 0.9760095753538718\n",
      "epoch 5 batch id 1401 loss 0.054756347090005875 train acc 0.9762000356887938\n",
      "epoch 5 batch id 1601 loss 0.03840545564889908 train acc 0.9767235321673954\n",
      "epoch 5 batch id 1801 loss 0.10762347280979156 train acc 0.9772001665741255\n",
      "epoch 5 batch id 2001 loss 0.04991726577281952 train acc 0.9773160294852574\n",
      "epoch 5 batch id 2201 loss 0.10824605077505112 train acc 0.977538618809632\n",
      "\n",
      "epoch 5 train acc 0.9777690379692833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02feba82ed0549f5808eb0b996f8fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=782.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 test acc 0.8963395140664961\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "naver_review_classifications_pytorch_kobert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
